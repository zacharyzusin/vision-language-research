dataset:
  root: data/iNat2021
  version: 2021

model:
  # Much better for fine-grained than ViT-B/32
  clip_model: ViT-B/16

  # Number of sub-prompts per class
  K: 32

  # EM temperature schedule
  # em_tau_start: initial temperature (softer assignments)
  em_tau_start: 1.0
  # em_tau_end: final temperature (sharper, more specialized)
  # Increased to 0.7 to keep assignments softer and prevent collapse to single sub-prompt
  # Higher temperature = softer assignments = better spreading across sub-prompts
  em_tau_end: 0.7

  # Diversity and entropy regularization (to prevent mode collapse)
  diversity_loss_weight: 1.0  # Strong penalty for similar sub-prompts (ensures feature diversity)
  entropy_loss_weight: 0.15   # Forces spreading of assignments across sub-prompts
  
  # Minimum usage loss: force ALL sub-prompts to be used (optional, default disabled)
  min_usage_loss_weight: 0.0  # Set to >0 to force all K sub-prompts to be used (e.g., 5.0)
  min_usage_threshold: 0.05   # Minimum average gamma per sub-prompt to avoid penalty

train:
  batch_size: 96       # increased for speed: with 12 GPUs and 128GB memory, effective batch=1152
  val_freq: 3          # validate every 3 epochs instead of every epoch (saves ~10-15 min per validation)
  lr: 5e-4             # lowered from 1e-3 (more stable for prompt tuning)
  epochs: 30
  warmup_steps: 2000
  lambda_mixture: 0.5  # weight for mixture loss vs classification loss
  temp_cls: 0.07       # temperature for classification logits
  num_workers: 12     # reduced from 24: system suggests max 16, using 12 to avoid overhead
  prefetch_factor: 4   # reduced from 8: balance prefetching with memory usage
  use_compile: true    # use torch.compile() for 20-30% speedup (PyTorch 2.0+)
