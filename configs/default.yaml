dataset:
  root: data/iNat2018
  version: 2018

model:
  clip_model: ViT-B/32

  # Shared prompt hyperparameters
  K: 32          # number of mixture prompts (more is better with shared prompts)
  ctx_len: 8     # number of learnable context tokens per prompt (CoOp uses 16)

  # EM temperature
  em_tau: 5.0  

train:
  batch_size: 16      # ViT-B/32 can handle this; improves stability
  lr: 1e-3             # shared prompts learn faster; 1e-3 is optimal in CoOp/MaPLe
  epochs: 30           # longer training improves generalization
  warmup_steps: 2000   # stabilize early EM iterations
