dataset:
  root: data/iNat2018
  version: 2018

model:
  # Much better for fine-grained than ViT-B/32
  clip_model: ViT-B/16

  # Number of sub-prompts per class
  K: 32

  # EM temperature schedule
  # em_tau_start: initial temperature (softer assignments)
  em_tau_start: 1.0
  # em_tau_end: final temperature (sharper, more specialized)
  em_tau_end: 0.3

train:
  batch_size: 16       # adjust based on GPU memory
  lr: 1e-3             # good starting point for prompt_offsets
  epochs: 30
  warmup_steps: 2000
